# Awesome-Vision-Language-Models

A curated list of large vision language models and related works.

## Foundation VLM Models

- `CLIP` Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.  
  [[Paper](https://arxiv.org/abs/2103.00020)] [[Code](https://github.com/OpenAI/CLIP)]  
- `ALIGN` Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. ICML 2021.  
  [[Paper](https://arxiv.org/abs/2102.05918)]
- `LiT` LiT: Zero-Shot Transfer with Locked-image text Tuning. CVPR 2022.   
  [[Paper](https://arxiv.org/abs/2111.07991)] [[Code](https://github.com/google-research/vision_transformer#lit-models)] 
- `SigLIP` Sigmoid Loss for Language Image Pre-Training. ICCV 2023.  
  [[Paper](https://arxiv.org/abs/2303.15343)] [[Code](https://github.com/google-research/big_vision)]  
- `AlphaCLIP` Alpha-CLIP: A CLIP Model Focusing on Wherever You Want. CVPR 2024.  
  [[Paper](https://arxiv.org/abs/2312.03818)] [[Code](https://github.com/SunzeY/AlphaCLIP)] 
- `LongCLIP` Long-CLIP: Unlocking the Long-Text Capability of CLIP. ECCV 2024.  
  [[Paper](https://arxiv.org/abs/2403.15378)] [[Code](https://github.com/beichenzbc/Long-CLIP)]
- `CLIP-Refine` Post-pre-training for Modality Alignment in Vision-Language Foundation Models. CVPR 2025.   
  [[Paper](https://arxiv.org/abs/2504.12717)] [[Code](https://github.com/yshinya6/clip-refine)]
- `MSTA`Efficient Transfer Learning for Video-language Foundation Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2411.11223)] [[Code](https://github.com/chenhaoxing/ETL4Video)]    
- `TAPT` TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2411.13136)] [Code(Not provided)]   
- `SAIL` Assessing and Learning Alignment of Unimodal Vision and Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2412.04616)] [[Code](https://lezhang7.github.io/sail.github.io/)]   
- `StatA` Realistic Test-Time Adaptation of Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2501.03729)] [[Code](https://github.com/MaxZanella/StatA)]
- `2SFS` Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.11609)] [[Code](https://github.com/FarinaMatteo/rethinking_fewshot_vlms)]
- `GDE` Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.17142)] [[Code](https://github.com/BerasiDavide/vlm_image_compositionality)]   
- `O-TPT` O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.12096)] [[Code](https://github.com/ashshaksharifdeen/O-TPT)]   


## Large VLM Models

- `BLIP` BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. ICML 2022.   
  [[Paper](https://arxiv.org/abs/2201.12086)] [[Code](https://github.com/salesforce/BLIP)]
- `BLIP2` BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. ICML 2023.   
  [[Paper](https://arxiv.org/abs/2301.12597)] [[Code](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]   
- `LLaVA` Visual Instruction Tuning.  NeurIPS 2023.  
  [[Paper](https://arxiv.org/abs/2304.08485)] [[Code](https://github.com/haotian-liu/LLaVA)]   
- `LLaVa-1.5` Improved Baselines with Visual Instruction Tuning. CVPR 2024.   
  [[Paper](https://arxiv.org/abs/2310.03744)] [[Code](https://github.com/haotian-liu/LLaVA)]   
- `Prismatic VLMs` Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models. ICML 2024.   
  [[Paper](https://arxiv.org/abs/2402.07865)] [[Code](https://github.com/TRI-ML/prismatic-vlms)]
- `MoVE-KD` MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders. CVPR 2025.   
  [[Paper](https://arxiv.org/abs/2501.01709)] [[Code](https://github.com/hey-cjj/MoVE-KD)]   
-  Cross-modal Information Flow in Multimodal Large Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2411.18620)] [[Code](https://github.com/FightingFighting/cross-modal-information-flow-in-MLLM)]   
- `SoFA` Identifying and Mitigating Position Bias of Multi-image Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.13792)] [Code(Not Provided)]   
- `Perception Tokens` Perception Tokens Enhance Visual Reasoning in Multimodal Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2412.03548)] [[Code](https://github.com/mahtabbigverdi/Aurora-perception)]     

## CoT

- `LLaVa-CoT` LLaVA-CoT: Let Vision Language Models Reason Step-by-Step. arxiv.   
  [[Paper](https://arxiv.org/abs/2411.10440)] [[Code](https://github.com/PKU-YuanGroup/LLaVA-CoT)]   
- `CoMT` CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models. AAAI 2025.   
  [[Paper](https://arxiv.org/abs/2412.12932)]

## Reasoning
- `Critic-V` Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning. CVPR 2025.  
[[Paper](https://arxiv.org/abs/2411.18203)][[Code](https://github.com/kyrieLei/Critic-V)]

## Hallucination
- `Octopus` Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding. CVPR 2025.   
[[Paper](https://www.arxiv.org/abs/2503.00361)] [[Code](https://github.com/LijunZhang01/Octopus)]   

## VLM for Videos

- `Valley` Valley: Video Assistant with Large Language model Enhanced abilitY. arxiv 23.   
[[Paper](https://arxiv.org/abs/2306.07207)] [[Code](https://github.com/bytedance/Valley)]   
- `Valley2` Valley2: Exploring Multimodal Models with Scalable Vision-Language Design. arxiv 25.    
[[Paper](https://arxiv.org/abs/2501.05901)] [[Code](https://github.com/bytedance/Valley)]   
- `VIDHALLUC` VIDHALLUC: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2412.03735)] [[Code](https://people-robots.github.io/vidhalluc/)]    
- `Video-XL` Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2409.14485)] [[Code](https://github.com/VectorSpaceLab/Video-XL)]   

## Segmentation

- `LISA` LISA: Reasoning Segmentation via Large Language Model. CVPR 2024.   
[[Paper](https://arxiv.org/abs/2308.00692)] [[Code](https://github.com/dvlab-research/LISA?tab=readme-ov-file)]    
- `LISA++` LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model. arxiv 2023.   
[[Paper](https://arxiv.org/abs/2312.17240)] [[Code](https://github.com/dvlab-research/LISA?tab=readme-ov-file)]   
- `VISA` VISA: Reasoning Video Object Segmentation via Large Language Models. ECCV 24.   
[[Paper](https://arxiv.org/abs/2407.11325)] [[Code](https://github.com/cilinyan/VISA?tab=readme-ov-file)]   
- `ExCEL` Exploring CLIPâ€™s Dense Knowledge for Weakly Supervised Semantic Segmentation. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.20826)][[Code](https://github.com/zwyang6/ExCEL)]   
- `ResCLIP` ResCLIP: Residual Attention for Training-free Dense Vision-language Inference. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2411.15851)] [[Code](https://github.com/yvhangyang/ResCLIP)]

## Compression & Acceleration
- `ToMe` Token Merging: Your ViT But Faster. ICLR 2023.   
[[Paper](https://arxiv.org/abs/2210.09461)] [[Code](https://github.com/facebookresearch/ToMe)]   
- `FastV` An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models. ECCV 2024.   
[[Paper](https://arxiv.org/abs/2403.06764)] [[Code](https://github.com/pkunlp-icler/FastV)]   
- `SparseVLM` SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference. ICML 2025.   
[[Paper](https://arxiv.org/abs/2410.04417)] [[Code](https://github.com/Gumpest/SparseVLMs)]   
- `VoCo` VoCo-LLaMA: Towards Vision Compression with Large Language Models. CVPR 2025.    
[[Paper](https://arxiv.org/abs/2406.12275v2)] [[Code](https://github.com/Yxxxb/VoCo-LLaMA)]    
- `FastVLM` FastVLM: Efficient Vision Encoding for Vision Language Models. CVPR 2025.    
[[Paper](https://arxiv.org/abs/2412.13303)] [[Code](https://github.com/apple/ml-fastvlm)]    
- `SGL` A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs. CVPR 2025.     
[[Paper](https://arxiv.org/abs/2412.03324)] [[Code](https://github.com/NUS-HPC-AI-Lab/SGL)]   
- `Pyramid Drop` PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2410.17247)] [[Code](https://github.com/Cooperx521/PyramidDrop)]   

arxiv:
- `PruMerge` LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models. arxiv 24.
[[Paper](https://arxiv.org/abs/2403.15388)] [[Code](https://llava-prumerge.github.io/)]   
- `YOPO` Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See. arxiv 24.   
[[Paper](https://arxiv.org/abs/2410.06169)] [[Code](https://github.com/ZhangAIPI/YOPO_MLLM_Pruning)]   
- `LRS-VQA` When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning. arxiv 25.   
[[Paper](https://arxiv.org/abs/2503.07588)] [[Code](https://github.com/VisionXLab/LRS-VQA)]   

## Medical Image
- `BrainMVP` Multi-modal Vision Pre-training for Medical Image Analysis. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2410.10604)] [[Code](https://github.com/openmedlab/BrainMVP)]   
- `BiomedCoOp` BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2411.15232)][[Code](https://github.com/HealthX-Lab/BiomedCoOp)]]   
- Task-Specific Knowledge Distillation from the Vision Foundation Model for Enhanced Medical Image Segmentation. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.06976)] [Code(Not Provided)]    

## Safety
- `HySAC` Hyperbolic Safety-Aware Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.12127)] [[Code](https://github.com/aimagelab/HySAC)]   

## Datasets

- `ShareGPT4V` ShareGPT4V: Improving Large Multi-Modal Models with Better Captions. ECCV 2024.   
  [[Paper](https://arxiv.org/abs/2311.12793)] [[Code & Dataset](https://github.com/ShareGPT4Omni/ShareGPT4V)]   

## Benchmark
- `CoSpace` CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2503.14161)] [[Code](https://thunlp-mt.github.io/CoSpace/)]   
- `LLaVA-Critic` LLaVA-Critic: Learning to Evaluate Multimodal Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2410.02712)] [[Code](https://github.com/LLaVA-VL/LLaVA-NeXT)]   


