# Awesome-Large-Vision-Language-Models

A curated list of large vision language models and related works.

## Foundation VLM Models

- `CLIP` **Learning Transferable Visual Models From Natural Language Supervision.** ICML 2021.  
  [[Paper](https://arxiv.org/abs/2103.00020)] [[Code](https://github.com/OpenAI/CLIP)]  
- `ALIGN` **Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision.** ICML 2021.  
  [[Paper](https://arxiv.org/abs/2102.05918)]
- `LiT` **LiT: Zero-Shot Transfer with Locked-image text Tuning.** CVPR 2022.   
  [[Paper](https://arxiv.org/abs/2111.07991)] [[Code](https://github.com/google-research/vision_transformer#lit-models)] 
- `SigLIP` **Sigmoid Loss for Language Image Pre-Training.** ICCV 2023.  
  [[Paper](https://arxiv.org/abs/2303.15343)] [[Code](https://github.com/google-research/big_vision)]  
- `AlphaCLIP` **Alpha-CLIP: A CLIP Model Focusing on Wherever You Want.** CVPR 2024.  
  [[Paper](https://arxiv.org/abs/2312.03818)] [[Code](https://github.com/SunzeY/AlphaCLIP)] 
- `LongCLIP` **Long-CLIP: Unlocking the Long-Text Capability of CLIP.** ECCV 2024.  
  [[Paper](https://arxiv.org/abs/2403.15378)] [[Code](https://github.com/beichenzbc/Long-CLIP)]

## Large VLM Models

- `BLIP` **BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.** ICML 2022.   
  [[Paper](https://arxiv.org/abs/2201.12086)] [[Code](https://github.com/salesforce/BLIP)]
- `BLIP2` **BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.** ICML 2023.   
  [[Paper](https://arxiv.org/abs/2301.12597)] [[Code](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]   
- `LLaVA` **Visual Instruction Tuning.**  NeurIPS 2023.  
  [[Paper](https://arxiv.org/abs/2304.08485)] [[Code](https://github.com/haotian-liu/LLaVA)]   
- `LLaVa-1.5` **Improved Baselines with Visual Instruction Tuning.** CVPR 2024.   
  [[Paper](https://arxiv.org/abs/2310.03744)] [[Code](https://github.com/haotian-liu/LLaVA)]   
- `Prismatic VLMs` **Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models.** ICML 2024.   
  [[Paper](https://arxiv.org/abs/2402.07865)] [[Code](https://github.com/TRI-ML/prismatic-vlms)]

## CoT

- `LLaVa-CoT` **LLaVA-CoT: Let Vision Language Models Reason Step-by-Step.** arxiv.   
  [[Paper](https://arxiv.org/abs/2411.10440)] [[Code](https://github.com/PKU-YuanGroup/LLaVA-CoT)]   
- `CoMT` **CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models.** AAAI 2025.   
  [[Paper](https://arxiv.org/abs/2412.12932)]


## VLM for Videos

- `Valley` **Valley: Video Assistant with Large Language model Enhanced abilitY.** arxiv 23.   
[[Paper](https://arxiv.org/abs/2306.07207)] [[Code](https://github.com/bytedance/Valley)]   
- `Valley2` **Valley2: Exploring Multimodal Models with Scalable Vision-Language Design.** arxiv 25.   
[[Paper](https://arxiv.org/abs/2501.05901)] [[Code](https://github.com/bytedance/Valley)]   

## Reasoning Segmentation

- `LISA` LISA: Reasoning Segmentation via Large Language Model. CVPR 2024.   
[[Paper](https://arxiv.org/abs/2308.00692)] [[Code](https://github.com/dvlab-research/LISA?tab=readme-ov-file)]    
- `LISA++` LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model. arxiv 23.   
[[Paper](https://arxiv.org/abs/2312.17240)] [[Code](https://github.com/dvlab-research/LISA?tab=readme-ov-file)]   
- `VISA` VISA: Reasoning Video Object Segmentation via Large Language Models. ECCV 24.   
[[Paper](https://arxiv.org/abs/2407.11325)] [[Code](https://github.com/cilinyan/VISA?tab=readme-ov-file)]   

### Compression
- `VoCo` VoCo-LLaMA: Towards Vision Compression with Large Language Models. CVPR 2025.   
[[Paper](https://arxiv.org/abs/2406.12275v2)] [[Code](https://github.com/Yxxxb/VoCo-LLaMA)]   


## Datasets

- `ShareGPT4V` **ShareGPT4V: Improving Large Multi-Modal Models with Better Captions.** ECCV 2024.   
  [[Paper](https://arxiv.org/abs/2311.12793)] [[Code & Dataset](https://github.com/ShareGPT4Omni/ShareGPT4V)]
